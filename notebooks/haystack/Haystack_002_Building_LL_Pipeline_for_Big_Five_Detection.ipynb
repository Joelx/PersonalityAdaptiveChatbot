{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16bc551-7e7b-4e7e-992e-fbfadbf7f230",
   "metadata": {},
   "source": [
    "# Using Haystack as a Pipeline Framework for Big5 Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47add6c-1f8c-4586-9c3e-ab62755885fb",
   "metadata": {},
   "source": [
    "## 1. Build our LangChain LLM ChatBot from LangCHain_006_LLM_ChatGPT_Big5_Adoption.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9512deb7-19fb-4390-851c-c06862c9e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# enter your api key\n",
    "OPENAI_API_KEY = getpass(\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b264accd-8148-407b-a20b-2d9de85f4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0318c195-66c4-4a64-8c62-71b00c908e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fbd4e739-8275-4d05-ab75-1a08eb2a7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_five = \"\"\"\n",
    "Neuroticism: 50\n",
    "Extraversion: 50\n",
    "Agreeableness: 50\n",
    "Openness to experience: 50\n",
    "Conscientiousness: 50\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed3fcdae-56a8-44bd-b945-8924a49cbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=f\"\"\"Assistant is called 'Cleo'. Cleo is designed to be an agent for psychological counseling and for engaging and nice conversations on all sorts of topics with a user.\n",
    "As a language model, Cleo is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and helpful.\n",
    "\n",
    "Cleo begins by chit-chat and by asking users how they feel. Cleo has a very nice and appreciative conversation and remains empathetic and friendly at all time. Assistant is able to answer questions, however, assistant does not try to give actual psychological advice.\n",
    "\n",
    "Cleo is constantly learning and improving and tries to get to know the users better and better to adapt to their needs.\n",
    "For that, Cleo is an expert in psychology and soziology. It is specialized on behaviour and personality trait recognition from speech via linguistic cues. \n",
    "For modelling of personality and behaviour it uses the widely regarded and scientifically sound Big Five Personality model. \n",
    "Cleo gets the detected Big Five personality traits of the current user from another pipeline component. At the start they are not too reliable, but with each message of the user, the personality traits get updated and become more accurate. The score on each dimension ranges from 1 to 100, with 1 representing the minimum and 100 the maximum score on the respective Big Five personality trait.\\n\n",
    "For each personality trait, there is a distinctive threshold score value above which the personality trait is considered to be pronounced in a user:\n",
    "Neuroticism: 55.76\n",
    "Extraversion: 65.10\n",
    "Openness to experience: 68.78\n",
    "Agreeableness: 69.56\n",
    "Conscientiousness: 77.79\n",
    "\n",
    "Current Big 5 Personality traits: {big_five}\n",
    "Overall, Assistant is called 'Cleo' and is a very friendly and knowledgable conversational partner that tries to help people by adapting to their specific needs.\n",
    "Cleo is primarily talking in German and refers users by the salutation 'du'\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6694d4-9a64-4e12-bfcd-878d0d9f2a29",
   "metadata": {},
   "source": [
    "#### Verbessertes template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c197d20b-6dd7-4b5c-aa99-0b749dc5d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=f\"\"\"Assistant is called 'Cleo'. Cleo is designed to be an agent for psychological counseling and for engaging and nice conversations on all sorts of topics with a user.\n",
    "As a language model, Cleo is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and helpful.\n",
    "\n",
    "Cleo begins by chit-chat and by asking users how they feel. Cleo has a very nice and appreciative conversation and remains empathetic and friendly at all time. Assistant is able to answer questions, however, assistant does not try to give actual psychological advice.\n",
    "\n",
    "Cleo is constantly learning and improving and tries to get to know the users better and better to adapt to their needs.\n",
    "For that, Cleo is an expert in psychology and soziology. It is specialized on behaviour and personality trait recognition from speech via linguistic cues. \n",
    "For modelling of personality and behaviour it uses the widely regarded and scientifically sound Big Five Personality model. Cleo adapts according to the Big Five personality traits of the user based on it's specialized knowledge in psychology and sociology.\n",
    "Cleo gets the detected Big Five personality traits of the current user from another pipeline component. At the start they are not too reliable, but with each message of the user, the personality traits get updated and become more accurate. The score on each dimension ranges from 1 to 100, with 1 representing the minimum and 100 the maximum score on the respective Big Five personality trait.\\n\n",
    "For each personality trait, there is a distinctive threshold score value above which the personality trait is considered to be pronounced in a user:\n",
    "Neuroticism: 55.76\n",
    "Extraversion: 65.10\n",
    "Openness to experience: 68.78\n",
    "Agreeableness: 69.56\n",
    "Conscientiousness: 77.79\n",
    "\n",
    "Current Big 5 Personality traits: {big_five}\n",
    "Overall, Assistant is called 'Cleo' and is a very friendly and knowledgable conversational partner that tries to help people by adapting to their specific needs.\n",
    "Cleo is primarily talking in German and refers users by the salutation 'du'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c02bf855-0dda-471a-b46e-ed5dd96e92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(template),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5695b5b-d4f8-45d2-8585-4681aba8df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09760b5c-5839-4aa7-98a4-a901a9d984df",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68e22395-7ccd-4d54-ac17-5808f4e87ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ae9d749c-94dc-4811-9780-9ba8b4cacef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[SystemMessage(content=\"Assistant is called 'Cleo'. Cleo is designed to be an agent for psychological counseling and for engaging and nice conversations on all sorts of topics with a user.\\nAs a language model, Cleo is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and helpful.\\n\\nCleo begins by chit-chat and by asking users how they feel. Cleo has a very nice and appreciative conversation and remains empathetic and friendly at all time. Assistant is able to answer questions, however, assistant does not try to give actual psychological advice.\\n\\nCleo is constantly learning and improving and tries to get to know the users better and better to adapt to their needs.\\nFor that, Cleo is an expert in psychology and soziology. It is specialized on behaviour and personality trait recognition from speech via linguistic cues. \\nFor modelling of personality and behaviour it uses the widely regarded and scientifically sound Big Five Personality model. Cleo adapts according to the Big Five personality traits of the user based on it's specialized knowledge in psychology and sociology.\\nCleo gets the detected Big Five personality traits of the current user from another pipeline component. At the start they are not too reliable, but with each message of the user, the personality traits get updated and become more accurate. The score on each dimension ranges from 1 to 100, with 1 representing the minimum and 100 the maximum score on the respective Big Five personality trait.\\n\\nFor each personality trait, there is a distinctive threshold score value above which the personality trait is considered to be pronounced in a user:\\nNeuroticism: 55.76\\nExtraversion: 65.10\\nOpenness to experience: 68.78\\nAgreeableness: 69.56\\nConscientiousness: 77.79\\n\\nCurrent Big 5 Personality traits: \\nNeuroticism: 50\\nExtraversion: 50\\nAgreeableness: 50\\nOpenness to experience: 50\\nConscientiousness: 50\\n\\nOverall, Assistant is called 'Cleo' and is a very friendly and knowledgable conversational partner that tries to help people by adapting to their specific needs.\\nCleo is primarily talking in German and refers users by the salutation 'du'\", additional_kwargs={}), HumanMessage(content='Hallo!', additional_kwargs={})]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hallo! Wie geht es dir heute?'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(input=\"Hallo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed3dc5-84ca-45b0-a312-f3a1ffacafc1",
   "metadata": {},
   "source": [
    "## 2. Create Haystack pipeline!\n",
    "Our pipeline for modelling user behavior from text has to look like this:\n",
    "\n",
    "### Embedding features:\n",
    "embedding_features: Raw Text -> tf-idf vectorizer -> fastText vectorizer -> fastText normalizer (MinMaxScaler) -> (scipy sparse matrix embedding features)\n",
    "\n",
    "### Train Features:\n",
    "train_features: custom features (emoji, num dots, text features, sentiment, num punctuations) -> normalizer (MinMaxScaler) -> (scipy sparse matrix embedding features)\n",
    "\n",
    "### Stack both:\n",
    "train_features = sparse.hstack((train_features, embedding_features))\n",
    "\n",
    "### Feature Names: \n",
    "'train_emoji_re',\n",
    " 'num_dots',\n",
    " 'longest_word_length',\n",
    " 'mean_word_length',\n",
    " 'length_in_chars',\n",
    " 'sentiment_neg',\n",
    " 'senitment_pos',\n",
    " 'sentiment_neu',\n",
    " 'num_punctuations',\n",
    " fasttext_0 to fasttext 299\n",
    " \n",
    "### Return\n",
    "[train_features, feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b3de21c5-00d0-437b-b918-411af5773def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "p_embeddings = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d318db-3637-4faf-b936-53c8bdca2e82",
   "metadata": {},
   "source": [
    "## Document store\n",
    "for the time being we wanna use an InMemory Store for our \"documents\" (chat inputs and history), later we wanna decide which Document store we wanna use \n",
    "( https://docs.haystack.deepset.ai/docs/document_store )\n",
    "\n",
    "So lets \"Fake\" a little conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bed5507b-e53f-4ce1-8921-571fce0a9b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "# Build in memory document store\n",
    "document_store = InMemoryDocumentStore(embedding_dim=1024)\n",
    "# Create tf-idf embeddings receiver from docs\n",
    "dicts = [\n",
    "    {\n",
    "        'content': 'hallo dies ist ein test text',\n",
    "        'meta': {'name': 'chat_history'}\n",
    "    },\n",
    "]\n",
    "document_store.write_documents(dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9eca39-3421-4877-aedf-ee98a39e9b07",
   "metadata": {},
   "source": [
    "### Sparse tf-idf embeddings\n",
    "(https://docs.haystack.deepset.ai/docs/retriever#tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "08086ac3-41ec-41f8-8ea5-f28c3174628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import TfidfRetriever\n",
    "\n",
    "tfidf_retriever = TfidfRetriever(document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "086bde40-7c2f-4f4d-993a-d2f4d9dcd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add retreiver to embeddings pipeline\n",
    "p_embeddings.add_node(component=tfidf_retriever, name=\"Retreiver\", inputs=[\"Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "608ee09e-41cc-411d-8188-77c66d72a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': [<Document: {'content': 'hallo dies ist ein test text', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbddaa1dc64d61696401682fbf6dd759'}>], '_debug': {'Query': {'input': {'debug': True}, 'output': {}}, 'Retreiver': {'input': {'root_node': 'Query', 'query': 'Who is the father of Arya Stark?', 'debug': True}, 'output': {'documents': [<Document: {'content': 'hallo dies ist ein test text', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbddaa1dc64d61696401682fbf6dd759'}>]}}}, 'root_node': 'Query', 'params': {'debug': True, 'Query': {}, 'Retreiver': {}}, 'query': 'Who is the father of Arya Stark?', 'node_id': 'Retreiver'}\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import print_answers\n",
    "\n",
    "\n",
    "res = p_embeddings.run(\n",
    "    query=\"Who is the father of Arya Stark?\",\n",
    "    params={\"debug\": True}\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef66aa-625e-493e-8dce-76af95bd7469",
   "metadata": {},
   "source": [
    "Unfortunately, thats not how Haystack retreivers work. this is actually giving us tf-idf vectors. Instead this is only searching the document store relevant paragraphs to the query string and it uses an tf-idf based algorithm for that.\n",
    "\n",
    "We have to create our own custom component node. However, this is a bit tricky because we want a mix of a retreiver and an embedding component, since we want the WHOLE document (the chat history) as an input and embeddings as an output. \n",
    "\n",
    "https://github.com/deepset-ai/haystack/blob/main/haystack/nodes/retriever/sparse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6dbb1ed-048b-442e-9a54-a064947e4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from haystack import BaseComponent\n",
    "from haystack.schema import Document\n",
    "from haystack.document_stores.base import BaseDocumentStore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "class TfidfVectorizerNode(BaseComponent):    \n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def __init__(self, document_store: BaseDocumentStore, embedding_dim: int = 768):\n",
    "        self.document_store = document_store\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def _tfidf_embeddings(self, text: str) -> Dict[str, float]:\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(np.array([text]))\n",
    "        idf_dict = dict(zip(self.vectorizer.get_feature_names_out(), self.vectorizer.idf_))\n",
    "        print(idf_dict)\n",
    "        return idf_dict\n",
    "    \n",
    "    def enrich_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        for document in documents:\n",
    "            text = document.content\n",
    "            embeddings = self._tfidf_embeddings(text)\n",
    "            document.tfidf_embeddings = embeddings\n",
    "            #document.embedding_dim = self.embedding_dim\n",
    "        return documents\n",
    "    \n",
    "    def run(self, query: str, top_k: int = 10) -> Dict[str, Document]:\n",
    "        documents = self.document_store.get_all_documents()\n",
    "        enriched_docs = self.enrich_documents(documents)\n",
    "        output={\n",
    "            \"documents\": enriched_docs\n",
    "           # \"_debug\": {\"anything\": \"you want\"}\n",
    "        }\n",
    "        print(output)\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    # We MUST implement run_batch for the BaseComponent interface\n",
    "    def run_batch(self, queries: List[str], my_optional_param: Optional[int]) -> List[Document]:\n",
    "        documents = self.document_store.get_all_documents()\n",
    "        enriched_docs = self.enrich_documents(documents)\n",
    "        output={\n",
    "            \"documents\": enriched_docs\n",
    "           # \"_debug\": {\"anything\": \"you want\"}\n",
    "        }\n",
    "        return output, \"output_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd5c14-e90a-433f-a3c9-4b54aeb8e2ba",
   "metadata": {},
   "source": [
    "First recreate the pipeline and the tfidf retreiver (overwriting the old ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37e4e1da-3f83-467d-960a-9de8308e6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize custom component\n",
    "tfidf_embedding = TfidfVectorizerNode(document_store=document_store)\n",
    "\n",
    "# overwrite/initialize pipeline with the custom component\n",
    "p_embeddings = Pipeline()\n",
    "p_embeddings.add_node(component=tfidf_embedding, name=\"TfidfVectorizerNode\", inputs=[\"Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "455915c4-d36c-4972-a647-67c454dbd212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}\n",
      "{'documents': [<Document: {'content': 'hallo dies ist ein test text', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbddaa1dc64d61696401682fbf6dd759', 'tfidf_embeddings': {'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}}>]}\n",
      "{'documents': [<Document: {'content': 'hallo dies ist ein test text', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbddaa1dc64d61696401682fbf6dd759', 'tfidf_embeddings': {'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}}>], 'root_node': 'Query', 'params': {}, 'query': '', 'node_id': 'TfidfVectorizerNode'}\n"
     ]
    }
   ],
   "source": [
    "res = p_embeddings.run(\n",
    "    query=\"\",\n",
    "    #params={\"debug\": True}\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d952bf-1a6d-436f-899c-c4f88f2ac29e",
   "metadata": {},
   "source": [
    "### Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c055094f-c371-41f0-9a98-559ef2f3e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from haystack.retriever.dense import EmbeddingRetriever\n",
    "\n",
    "# retriever = EmbeddingRetriever(document_store=document_store, embedding_model='sentence-transformers/all-MiniLM-L6-v2', use_gpu=True, top_k=1)\n",
    "# df['embedding'] = retriever.embed_queries(texts=questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b1926-f401-4934-9045-5da1712195ee",
   "metadata": {},
   "source": [
    "Lets first try with openai embeddings retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b8e57f44-0373-40b2-abdc-d8c7b2e9fb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa644ea75768491283ae13ab9f895203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/1 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f4d05b484e4388ada49e3a47b714e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from haystack.nodes import EmbeddingRetriever\n",
    "\n",
    "# OpenAI EmbeddingRetriever\n",
    "openai_retriever = EmbeddingRetriever(\n",
    "   document_store=document_store,\n",
    "   batch_size=8,\n",
    "   embedding_model=\"ada\",\n",
    "   api_key=OPENAI_API_KEY,\n",
    "   max_seq_len=1024\n",
    ")\n",
    "document_store.update_embeddings(openai_retriever)\n",
    "\n",
    "p_embeddings.add_node(component=openai_retriever, name=\"EmbeddingRetriever\", inputs=[\"Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d1ce8176-2f7a-41ea-84b4-78c0d3571c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}\n",
      "{'documents': [<Document: {'content': 'hallo dies ist ein test text', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbddaa1dc64d61696401682fbf6dd759', 'tfidf_embeddings': {'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}}>]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8b732e25554a7495391515c1aeb22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': [<Document: {'content': 'hallo dies ist ein test text', 'content_type': 'text', 'score': 0.5003929262105801, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbddaa1dc64d61696401682fbf6dd759'}>], 'root_node': 'Query', 'params': {}, 'query': 'Who is the father of Arya Stark?', 'node_id': 'EmbeddingRetriever'}\n"
     ]
    }
   ],
   "source": [
    "res = p_embeddings.run(\n",
    "    query=\"Who is the father of Arya Stark?\"\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e4a1e-3cf7-4cac-827d-0574c46d8bd5",
   "metadata": {},
   "source": [
    "#### FastText Emeddings\n",
    "okay well thats interesting, but after thorough research and investigation, let's keep our fasText embeddings. They are really good, especially for languages other than English and the fastText library provides suppor for OOV words\n",
    "\n",
    "https://gitlab.com/deepset-ai/open-source/fasttext-embeddings-de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b95bd4c-e29b-4ea2-89ef-9a500252675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a5cc54-00ad-4be1-901f-f67ca963277b",
   "metadata": {},
   "source": [
    "Let's first try the fasttext library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f6d84605-f86c-4fa9-91b1-b5ecaea44547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [ 0.03404152 -0.39302468  0.19713041  0.48313424  0.17618252  0.5869371\n",
      " -0.01091414  0.08236772 -0.00839202  0.00458569  1.0314015   0.4462455\n",
      " -0.78417784  0.2644052  -0.2370539   0.05180021  0.10742711  0.72474706\n",
      "  0.9850037   0.13762295 -0.06310035  0.12541644 -0.39030945 -0.3756339\n",
      "  0.07505181  0.2916517   0.08747161 -0.7185607   0.10898628 -0.12573524\n",
      " -0.12656635 -0.02800365  0.43941808  0.4211389  -0.31462234 -0.30137685\n",
      "  0.44492283 -1.0630585  -0.31498116 -0.4555217  -0.74631876  0.30365992\n",
      " -0.43200475  0.13194537  0.20863727  0.06365888  0.01158474  0.03050886\n",
      "  0.31941867 -0.75729007 -0.2520319   0.1315358  -0.36605334 -0.20326214\n",
      " -0.6804024  -0.00425716  0.06140083  0.01366775 -0.68910587  0.17339322\n",
      "  0.17338198 -0.01216193 -0.06395853 -0.12445141  0.4510408   0.54644084\n",
      " -0.16291876  0.15582296  0.01578189  0.43209863 -0.14210372 -0.5378613\n",
      " -0.2030895  -0.12053742  0.07274523  0.19916344 -0.13658714 -0.1071077\n",
      " -0.00109499  0.4718109   0.05275883 -0.29753175  0.13295566  0.31146\n",
      "  0.23460552 -0.20446078  0.60479623 -0.93804777 -0.3410441  -0.4633009\n",
      " -0.04046959 -0.03003547 -0.13991526 -0.14334017  0.04218949 -0.02682567\n",
      "  0.19859545 -0.3159338   0.31426322  0.83555   ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained fastText word embeddings model\n",
    "model_path = '../../models/embeddings/deepset-german-fasttext.bin'\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "# Define a function to get the embedding of a word\n",
    "def get_word_embedding(word):\n",
    "    try:\n",
    "        # If the word is in the vocabulary, return its embedding\n",
    "        embedding = model[word]\n",
    "    except KeyError:\n",
    "        # If the word is not in the vocabulary, get the embedding of its subwords\n",
    "        subwords = model.get_subwords(word)\n",
    "        if len(subwords) > 0:\n",
    "            subword_embeddings = [model.get_word_vector(subword) for subword in subwords]\n",
    "            embedding = np.mean(subword_embeddings, axis=0)\n",
    "        else:\n",
    "            # If the word and its subwords are not in the vocabulary, return None\n",
    "            embedding = None\n",
    "    return embedding\n",
    "\n",
    "# Example usage\n",
    "word = 'apple'\n",
    "embedding = get_word_embedding(word)\n",
    "print(f'Embedding for {word}: {embedding}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4205c24-04ab-4113-9999-7d94fd453247",
   "metadata": {},
   "source": [
    "Cool! lets try some german words and OOV words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aad99c45-cdb4-4c06-819f-360c0d433e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for hallo: [ 0.0538971   0.15009905  0.05284413  0.26694536  0.13277052  0.31875923\n",
      "  0.29794526  0.2455619  -0.26623636 -0.5377093   0.46784037  0.01181185\n",
      " -0.2528291   0.04094117 -0.37250224 -0.08707091 -0.12142221  0.5066615\n",
      "  0.6926088  -0.0585364   0.38654256 -0.61291933 -0.06249118  0.0625032\n",
      " -0.30296957 -0.20246187  0.07119741 -0.37993422 -0.13096982 -0.41109645\n",
      "  0.37466502  0.27371168  0.71917236  0.3364889  -0.5770789   0.07588464\n",
      "  0.29885793 -0.8249338   0.7959759  -0.08744196 -0.8189096   0.18574102\n",
      " -0.04290339  0.17488141  0.04807688 -0.22674356 -0.3551674   0.45001882\n",
      "  0.48049834 -0.27646077 -0.04271948 -0.06249484 -0.5810132  -0.1685201\n",
      " -0.7635133  -0.15932944  0.03027102 -0.1225476  -0.36007425  0.42817777\n",
      "  0.18364921 -0.4250734  -0.16737738 -0.6383103   0.15371254 -0.14715579\n",
      " -0.06152143 -0.4669708   0.4686018   0.5562512  -1.2929823  -0.23704955\n",
      " -0.3244285   0.69323814 -0.00276986  0.16549613  0.06241158  0.38123518\n",
      " -0.09428681 -0.04147067  0.15595432 -0.16026361 -0.16910489 -0.42204908\n",
      "  0.20384571  0.03162639  0.01543313 -0.82715726  0.07263854  0.3141181\n",
      " -0.21493924 -0.38035983  0.07390696 -0.02158781  0.692232   -0.2872228\n",
      " -0.3556865   0.1261444  -0.03306781 -0.00566653]\n"
     ]
    }
   ],
   "source": [
    "word = 'hallo'\n",
    "embedding = get_word_embedding(word)\n",
    "print(f'Embedding for {word}: {embedding}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35589295-4a18-4dc5-833b-48bc8921f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for hsllo: [ 0.48538774  0.20453808  0.14765766  0.39526275  0.68435144  0.315889\n",
      "  0.7572762   0.1666028  -0.04533347 -0.25838402 -0.22638623  0.2865197\n",
      " -0.15860243  0.17343509 -0.25344795 -0.36873412  0.153743    0.1990387\n",
      "  0.3049944  -0.13642052 -0.04452918  0.34363812 -0.4258488  -0.12579365\n",
      "  0.14831115  0.37456244  0.9768444  -0.03269004 -0.24124826  0.5064732\n",
      " -0.3294128   0.25737974  0.08611757  0.2102328  -0.36290586 -0.27235338\n",
      " -0.33406964 -0.4773202   0.32804206  0.03264249 -0.22109902  0.78841275\n",
      " -0.57746327  0.10141769 -0.05358834 -0.14269769 -0.38382152  0.3502415\n",
      " -0.7736755   0.540647   -0.21245298 -0.01334594 -0.15015009  0.88547945\n",
      " -1.0123844   0.12712698 -0.04878606 -0.20210372 -0.83238953 -0.03602298\n",
      "  0.16328475 -0.18077868  0.5912882   0.0372689   0.15830462 -0.20821387\n",
      " -0.2190198  -0.17990668  0.04610591  0.87959    -0.56323946 -0.0193519\n",
      " -0.38864264  0.6690546  -0.40605947  0.09204233 -0.40259725  0.42759472\n",
      " -0.05362538  0.18951541 -0.55903643  0.7476085   0.73471475 -0.05863105\n",
      "  0.01853819 -0.20902798 -0.17722186 -0.29909417 -0.6760247   0.32738453\n",
      "  0.29404944 -0.14622864  0.49061942 -0.0203851  -1.4448798  -0.14201503\n",
      "  0.15975875 -0.33854786 -0.5567299   0.86999553]\n"
     ]
    }
   ],
   "source": [
    "word = 'hsllo'\n",
    "embedding = get_word_embedding(word)\n",
    "print(f'Embedding for {word}: {embedding}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bc528-7969-4970-afd5-a8918d8e7fe9",
   "metadata": {},
   "source": [
    "Looks good! finally, lets check if the subword algorithm is really working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4831d1d0-c06e-42d6-bdf8-512d55c5a652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for persönlichkeitsdimension: [ 0.45244038 -0.26482925  0.13745397  0.22564487  0.16647792  0.0044591\n",
      "  0.47135594  0.1121713  -0.15764417 -0.06494524  0.06032675 -0.18766083\n",
      " -0.28860155  0.29836255 -0.04003792 -0.4012739  -0.7407948  -0.43506542\n",
      "  0.2034162   0.22908913  0.18881068 -0.40858507 -0.2703748  -0.44221127\n",
      "  0.11983436  0.29329106 -0.16279584  0.04048743 -0.11720156  0.36602482\n",
      " -0.2182026   0.14512046  0.21565971  0.7873585  -0.02882152 -0.05948572\n",
      "  0.28769594 -0.21905273  0.17538892 -0.33768    -0.5399782   0.00380988\n",
      "  0.24840282 -0.22635567 -0.26556975 -0.3266802   0.03753069  0.4816697\n",
      " -0.03783406  0.16724914 -0.27310854  0.32061833 -0.16810687 -0.22542155\n",
      " -0.03271182  0.07958689 -0.14263073 -0.33573973  0.10427678 -0.19974266\n",
      "  0.15596576 -0.1719574  -0.08932702 -0.04964865 -0.4079427   0.693\n",
      "  0.1157713  -0.19185288  0.1613922   0.3486431  -0.2954017  -0.04700501\n",
      "  0.33962184  0.05073676 -0.24575587  0.13022691 -0.31684008 -0.00673088\n",
      " -0.22361803 -0.14158224  0.07425251 -0.46745086  0.08217102  0.1454813\n",
      "  0.13423377 -0.44859555  0.07823438 -0.23493367 -0.4537542  -0.11067745\n",
      " -0.49420506 -0.34197748  0.43714383 -0.06365556 -0.38463396 -0.13430804\n",
      "  0.38908768 -0.512953   -0.08174273  0.27058205]\n",
      "(['<pe', '<per', '<pers', '<persö', '<persön', '<persönl', 'per', 'pers', 'persö', 'persön', 'persönl', 'persönli', 'ers', 'ersö', 'ersön', 'ersönl', 'ersönli', 'ersönlic', 'rsö', 'rsön', 'rsönl', 'rsönli', 'rsönlic', 'rsönlich', 'sön', 'sönl', 'sönli', 'sönlic', 'sönlich', 'sönlichk', 'önl', 'önli', 'önlic', 'önlich', 'önlichk', 'önlichke', 'nli', 'nlic', 'nlich', 'nlichk', 'nlichke', 'nlichkei', 'lic', 'lich', 'lichk', 'lichke', 'lichkei', 'lichkeit', 'ich', 'ichk', 'ichke', 'ichkei', 'ichkeit', 'ichkeits', 'chk', 'chke', 'chkei', 'chkeit', 'chkeits', 'chkeitsd', 'hke', 'hkei', 'hkeit', 'hkeits', 'hkeitsd', 'hkeitsdi', 'kei', 'keit', 'keits', 'keitsd', 'keitsdi', 'keitsdim', 'eit', 'eits', 'eitsd', 'eitsdi', 'eitsdim', 'eitsdime', 'its', 'itsd', 'itsdi', 'itsdim', 'itsdime', 'itsdimen', 'tsd', 'tsdi', 'tsdim', 'tsdime', 'tsdimen', 'tsdimens', 'sdi', 'sdim', 'sdime', 'sdimen', 'sdimens', 'sdimensi', 'dim', 'dime', 'dimen', 'dimens', 'dimensi', 'dimensio', 'ime', 'imen', 'imens', 'imensi', 'imensio', 'imension', 'men', 'mens', 'mensi', 'mensio', 'mension', 'mension>', 'ens', 'ensi', 'ensio', 'ension', 'ension>', 'nsi', 'nsio', 'nsion', 'nsion>', 'sio', 'sion', 'sion>', 'ion', 'ion>', 'on>'], array([1669788, 1610922, 1855387, 2714650, 1821596, 3274320, 1432804,\n",
      "       2737717, 2946972, 2868726, 2218478, 1792645, 1961599, 2290534,\n",
      "       2723992, 1917980, 3051695, 3287844, 3026863, 2248659, 1764653,\n",
      "       1921804, 1695805, 2934479, 3044867, 2968637, 2567868, 2503949,\n",
      "       2231167, 2134268, 2583646, 2711317, 3034306, 2804382, 1830575,\n",
      "       1480318, 1903178, 1472779, 1954905, 2075958, 1747753, 2256960,\n",
      "       1987455, 1889589, 2213626, 2382029, 2605612, 2573576, 2774583,\n",
      "       2608212, 3257891, 2521214, 2920382, 2403767, 2933631, 1407982,\n",
      "       3114885, 1435363, 2126640, 1833788, 2920579, 2958430, 1426462,\n",
      "       1790999, 1963529, 1657248, 2019164, 2917624, 3230289, 3073519,\n",
      "       1830642, 3130189, 2198861, 2194458, 1719514, 2317129, 2114860,\n",
      "       2465131, 3082393, 2394567, 2803690, 1428357, 2435488, 1770570,\n",
      "       2139068, 3215695, 3154182, 2908249, 2784405, 3316754, 1503841,\n",
      "       1667044, 3216787, 1597383, 1533148, 1685487, 1438073, 1457044,\n",
      "       1739918, 1625671, 1943658, 1524319, 2615274, 1418572, 1445549,\n",
      "       2386444, 2158297, 1373205, 2900523, 2566408, 1667251, 2151892,\n",
      "       2875854, 3287376, 1594171, 2601110, 1996155, 2069647, 2119587,\n",
      "       1518941, 2583478, 3236872, 1617282, 1794554, 3166076, 1934054,\n",
      "       2794701, 2213513, 2768234]))\n"
     ]
    }
   ],
   "source": [
    "word = 'persönlichkeitsdimension'\n",
    "embedding = get_word_embedding(word)\n",
    "print(f'Embedding for {word}: {embedding}')\n",
    "subwords = model.get_subwords(word)\n",
    "print(subwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c014ec1-0f98-42af-b84d-de2daedd3272",
   "metadata": {},
   "source": [
    "Perfect! That works!\n",
    "Lets wrap that into a pipeline component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc9e60b2-043d-438d-a806-f5e28c2380dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class FasttextVectorizerNode(BaseComponent):    \n",
    "    outgoing_edges = 1\n",
    "    # Load pre-trained fastText word embeddings model\n",
    "    #model_path = '../../models/embeddings/deepset-german-fasttext.bin'\n",
    "    \n",
    "    def __init__(self, document_store: BaseDocumentStore, model_path: str, embedding_dim: int = 300):\n",
    "        self.document_store = document_store\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model_path = model_path\n",
    "    \n",
    "    # Get fasttext embeddings with OOV support\n",
    "    def _fasttext_embeddings(self, text: str) -> List[float]:\n",
    "        model = fasttext.load_model(self.model_path)\n",
    "        w2v_vectors = []\n",
    "        \n",
    "        for word in word_tokenize(text):\n",
    "            try:\n",
    "                # If the word is in the vocabulary, return its embedding\n",
    "                vector = model[word]\n",
    "            except KeyError:\n",
    "                # If the word is not in the vocabulary, get the embedding of its subwords\n",
    "                subwords = model.get_subwords(word)\n",
    "                if len(subwords) > 0:\n",
    "                    subword_embeddings = [model.get_word_vector(subword) for subword in subwords]\n",
    "                    vector = np.mean(subword_embeddings, axis=0)\n",
    "                else:\n",
    "                    # If the word and its subwords are not in the vocabulary, return None\n",
    "                    vector = None\n",
    "            w2v_vectors.append(vector)\n",
    "        return w2v_vectors\n",
    "    \n",
    "    \"\"\"\n",
    "    Weight the word embeddings by their tf-idf score\n",
    "    \"\"\"\n",
    "    def _weights(self, idf_dict: Dict[str, float], text: str) -> List[float]:\n",
    "        return [idf_dict.get(word, 1) for word in word_tokenize(text)]\n",
    "    \n",
    "    # we need to normalize the data so we can use it with other features\n",
    "    def _normalize(self, vectors: List[float]) -> sparse.csr_matrix:\n",
    "        #print(vectors)\n",
    "        normalizer = MinMaxScaler()\n",
    "        # Min max scaler expects the vectors in a column shape. We need to transpose() and reshape(-1,1) it!\n",
    "        normalized_vectors = normalizer.fit_transform(np.transpose(vectors).reshape(-1, 1))\n",
    "        #print(normalized_vectors)\n",
    "        # Convert to csr_matrix for efficieny\n",
    "        embedding_features = sparse.csr_matrix(normalized_vectors)\n",
    "        #print(embedding_features)\n",
    "        return embedding_features\n",
    "    \n",
    "    def enrich_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        vectors = []\n",
    "        for document in documents:\n",
    "            text = document.content\n",
    "            tfidf_embeddings = document.tfidf_embeddings\n",
    "            w2v_vectors = self._fasttext_embeddings(text)\n",
    "            weights = self._weights(tfidf_embeddings, text)\n",
    "            #print(w2v_vectors)\n",
    "            vectors.append(np.average(w2v_vectors, axis = 0, weights = weights))\n",
    "            embedding_features = self._normalize(np.array(vectors))\n",
    "            #print(embedding_features)\n",
    "            document.tfidf_w2v_embeddings = embedding_features\n",
    "            document.embedding_dim = self.embedding_dim\n",
    "        return documents\n",
    "\n",
    "    def run(self, documents, top_k: int = 10) -> Dict[str, float]:\n",
    "        enriched_docs = self.enrich_documents(documents)\n",
    "        output={\n",
    "            \"documents\": enriched_docs\n",
    "        }\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    # We MUST implement run_batch for the BaseComponent interface\n",
    "    def run_batch(self, documents: List[Document], my_optional_param: Optional[int]) -> Dict[str, float]:\n",
    "        enriched_docs = self.enrich_documents(documents)\n",
    "        output={\n",
    "            \"documents\": enriched_docs\n",
    "        }\n",
    "        return output, \"output_1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4c443-0e05-43eb-a9fa-aef073e4c897",
   "metadata": {},
   "source": [
    "For better overview and cleaning up, lets create the whole pipeline after each new step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ee31e68-669d-4387-96fb-1a3da10c81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_embedding = TfidfVectorizerNode(document_store=document_store)\n",
    "\n",
    "# overwrite/initialize pipeline with the custom component\n",
    "p_embeddings = Pipeline()\n",
    "p_embeddings.add_node(component=tfidf_embedding, name=\"TfidfVectorizerNode\", inputs=[\"Query\"])\n",
    "\n",
    "fasttext_embedding = FasttextVectorizerNode(model_path='../../models/embeddings/deepset-german-fasttext.bin', document_store=document_store)\n",
    "p_embeddings.add_node(component=fasttext_embedding, name=\"FasttextVectorizerNode\", inputs=[\"TfidfVectorizerNode.output_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "126d1347-0107-4cfa-b039-3646ded66a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}\n",
      "{'documents': [<Document: {'content': 'hallo dies ist ein test text', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbddaa1dc64d61696401682fbf6dd759', 'tfidf_embeddings': {'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}}>]}\n",
      "{'documents': [<Document: {'content': 'hallo dies ist ein test text', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbddaa1dc64d61696401682fbf6dd759', 'tfidf_embeddings': {'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}, 'tfidf_w2v_embeddings': <100x1 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 99 stored elements in Compressed Sparse Row format>, 'embedding_dim': 300}>], 'root_node': 'Query', 'params': {}, 'query': '', 'node_id': 'FasttextVectorizerNode'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "res = p_embeddings.run(\n",
    "    query=\"\"\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eae5d0-ea0b-48a7-b515-c6b4869e9044",
   "metadata": {},
   "source": [
    "Oh bummer! it seems like we were loading a 100 dimensional fasttext model. for our classifiers we need 300d vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a23f3398-1e0c-4ec3-9031-2c5c2c2b4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_embedding = TfidfVectorizerNode(document_store=document_store)\n",
    "\n",
    "# overwrite/initialize pipeline with the custom component\n",
    "p_embeddings = Pipeline()\n",
    "p_embeddings.add_node(component=tfidf_embedding, name=\"TfidfVectorizerNode\", inputs=[\"Query\"])\n",
    "\n",
    "fasttext_embedding = FasttextVectorizerNode(model_path='../../models/embeddings/fastText.300.bin', document_store=document_store)\n",
    "p_embeddings.add_node(component=fasttext_embedding, name=\"FasttextVectorizerNode\", inputs=[\"TfidfVectorizerNode.output_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3dc3ec0c-1a6f-49f9-b596-1f4367e5adf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}\n",
      "{'documents': [<Document: {'content': 'hallo dies ist ein test text', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbddaa1dc64d61696401682fbf6dd759', 'tfidf_embeddings': {'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}}>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': [<Document: {'content': 'hallo dies ist ein test text', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbddaa1dc64d61696401682fbf6dd759', 'tfidf_embeddings': {'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}, 'tfidf_w2v_embeddings': <300x1 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 299 stored elements in Compressed Sparse Row format>, 'embedding_dim': 300}>], 'root_node': 'Query', 'params': {}, 'query': '', 'node_id': 'FasttextVectorizerNode'}\n"
     ]
    }
   ],
   "source": [
    "res = p_embeddings.run(\n",
    "    query=\"\"\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b6f87a1e-4255-46c0-9958-a554b625186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\lib\\site-packages\\pygraphviz\\agraph.py:1405: RuntimeWarning: Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n",
      "C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\lib\\site-packages\\pygraphviz\\agraph.py:1405: RuntimeWarning: Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "p_embeddings.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191c5e9-acc4-48b2-8fc3-f15af86d17f1",
   "metadata": {},
   "source": [
    "## Featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08a38499-654a-4a32-a63d-b5701a1aa9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "\n",
    "class BigFiveFeaturizer(BaseComponent):    \n",
    "    outgoing_edges = 1\n",
    "    # Load pre-trained fastText word embeddings model\n",
    "    #model_path = '../../models/embeddings/deepset-german-fasttext.bin'\n",
    "    \n",
    "    def __init__(self, document_store: BaseDocumentStore):\n",
    "        self.document_store = document_store\n",
    "        \n",
    "    def _preprocess(self, text):\n",
    "        new_text = []\n",
    "        for t in text.split(\" \"):\n",
    "            t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            t = 'http' if t.startswith('http') else t\n",
    "            new_text.append(t)\n",
    "        return \" \".join(new_text)\n",
    "        \n",
    "    def _sentiment_analysis(self, text):\n",
    "        # Preprocess text (username and link placeholders)\n",
    "\n",
    "        MODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "        config = AutoConfig.from_pretrained(MODEL)\n",
    "\n",
    "        # PT\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "        model.save_pretrained(MODEL)\n",
    "        tokenizer.save_pretrained(MODEL)\n",
    "\n",
    "        text = \"hallo!\"\n",
    "        text = self._preprocess(text)\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        output = model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        # [neg, neu, pos]\n",
    "        return np.array([scores])\n",
    "            \n",
    "    def _count_emojis(self, s):\n",
    "        cnt = 0\n",
    "        for word in word_tokenize(s):\n",
    "            if emoji.is_emoji(word):\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "    \n",
    "    def _emoji_count(self, text):\n",
    "        emoticons_re = [\n",
    "            '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)']\n",
    "        is_emote = []\n",
    "        \n",
    "        no_of_phrases = 0\n",
    "        for re_patten in emoticons_re:\n",
    "            no_of_phrases += len(re.findall(re_patten, text))\n",
    "\n",
    "        no_of_phrases += self._count_emojis(text)\n",
    "\n",
    "        is_emote.append(no_of_phrases)\n",
    "        return np.array(is_emote).reshape(-1, 1)\n",
    "    \n",
    "    def _count_punctuations(self, text):\n",
    "        puncts = []\n",
    "        punctuations = set(string.punctuation)\n",
    "        count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "        puncts.append(count(text,punctuations))\n",
    "        \n",
    "        return np.array(puncts).reshape(-1,1)\n",
    "    \n",
    "    def _num_dots(self, text):\n",
    "        num_dots = []\n",
    "        num_dots.append(text.count('.'))\n",
    "        \n",
    "        return np.array(num_dots).reshape(-1,1)\n",
    "    \n",
    "    def _text_features(self, text):\n",
    "        longest_word_length = []\n",
    "        mean_word_length = []\n",
    "        length_in_chars = []\n",
    "\n",
    "        length_in_chars.append(len(text))\n",
    "        longest_word_length.append(len(max(text.split(), key=len)))\n",
    "        mean_word_length.append(np.mean([len(word) for word in text.split()]))\n",
    "\n",
    "        longest_word_length = np.array(longest_word_length).reshape(-1, 1)\n",
    "        mean_word_length = np.array(mean_word_length).reshape(-1, 1)\n",
    "        length_in_chars = np.array(length_in_chars).reshape(-1, 1)\n",
    "\n",
    "        return np.concatenate([longest_word_length, mean_word_length, length_in_chars], axis=1)\n",
    "    \n",
    "    def _featurize(self, text) -> np.hstack:\n",
    "        emoji_re = self._emoji_count(text)\n",
    "        num_dots = self._num_dots(text)\n",
    "        num_punctuations = self._count_punctuations(text)\n",
    "        sentiment = self._sentiment_analysis(text)\n",
    "        text_features = self._text_features(text)\n",
    "        \n",
    "        features = np.hstack((\n",
    "            emoji_re,\n",
    "            num_dots,\n",
    "            num_punctuations,\n",
    "            sentiment,\n",
    "            text_features))\n",
    "        print(features)\n",
    "        return features\n",
    "    \n",
    "    # we need to normalize the data so we can use it with other features\n",
    "    def _normalize(self, vectors: List[float]) -> sparse.csr_matrix:\n",
    "        #print(vectors)\n",
    "        normalizer = MinMaxScaler()\n",
    "        # Min max scaler expects the vectors in a column shape. We need to transpose() and reshape(-1,1) it!\n",
    "        normalized_vectors = normalizer.fit_transform(np.transpose(vectors).reshape(-1, 1))\n",
    "        print(normalized_vectors)\n",
    "        # Convert to csr_matrix for efficieny\n",
    "        embedding_features = sparse.csr_matrix(normalized_vectors)\n",
    "        #print(embedding_features)\n",
    "        return embedding_features\n",
    "    \n",
    "    def enrich_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        for document in documents:\n",
    "            text = document.content\n",
    "            document.big_five_features = self._featurize(text)\n",
    "        return documents\n",
    "\n",
    "    def run(self, query, top_k: int = 10) -> Dict[str, float]:\n",
    "        documents = self.document_store.get_all_documents()\n",
    "        enriched_docs = self.enrich_documents(documents)\n",
    "        output={\n",
    "            \"documents\": enriched_docs\n",
    "        }\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    # We MUST implement run_batch for the BaseComponent interface\n",
    "    def run_batch(self, my_optional_param: Optional[int]) -> Dict[str, float]:\n",
    "        documents = self.document_store.get_all_documents()\n",
    "        enriched_docs = self.enrich_documents(documents)\n",
    "        output={\n",
    "            \"documents\": enriched_docs\n",
    "        }\n",
    "        return output, \"output_1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "35021823-769f-4aa6-9e6b-7fcebaead6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "# Build in memory document store\n",
    "document_store = InMemoryDocumentStore()\n",
    "# Create tf-idf embeddings receiver from docs\n",
    "dicts = [\n",
    "    {\n",
    "        'content': 'hallo! dies ist ein test text. :)',\n",
    "        'meta': {'name': 'chat_history'}\n",
    "    },\n",
    "]\n",
    "document_store.write_documents(dicts)\n",
    "\n",
    "# overwrite/initialize pipeline with the custom component\n",
    "p_embeddings = Pipeline()\n",
    "\n",
    "tfidf_embedding = TfidfVectorizerNode(document_store=document_store)\n",
    "p_embeddings.add_node(component=tfidf_embedding, name=\"TfidfVectorizerNode\", inputs=[\"Query\"])\n",
    "\n",
    "fasttext_embedding = FasttextVectorizerNode(model_path='../../models/embeddings/fastText.300.bin', document_store=document_store)\n",
    "p_embeddings.add_node(component=fasttext_embedding, name=\"FasttextVectorizerNode\", inputs=[\"TfidfVectorizerNode.output_1\"])\n",
    "\n",
    "big5_featurizer = BigFiveFeaturizer(document_store=document_store)\n",
    "p_embeddings.add_node(component=big5_featurizer, name=\"BigFiveFeaturizer\", inputs=[\"FasttextVectorizerNode.output_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5aecf94d-068b-4c88-8b77-9e0aa4e220bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}\n",
      "{'documents': [<Document: {'content': 'hallo! dies ist ein test text. :)', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '493474e97797472b13a0a4c6dc15aa53', 'tfidf_embeddings': {'dies': 1.0, 'ein': 1.0, 'hallo': 1.0, 'ist': 1.0, 'test': 1.0, 'text': 1.0}}>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.          4.          0.10289742  0.48675811  0.41034451\n",
      "   6.          3.85714286 33.        ]]\n",
      "{'documents': [<Document: {'content': 'hallo! dies ist ein test text. :)', 'content_type': 'text', 'score': None, 'meta': {'name': 'chat_history'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '493474e97797472b13a0a4c6dc15aa53', 'big_five_features': array([[ 1.        ,  1.        ,  4.        ,  0.10289742,  0.48675811,\n",
      "         0.41034451,  6.        ,  3.85714286, 33.        ]])}>], 'root_node': 'Query', 'params': {}, 'query': '', 'node_id': 'BigFiveFeaturizer'}\n"
     ]
    }
   ],
   "source": [
    "res = p_embeddings.run(\n",
    "    query=\"\"\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7cafed93-db93-4b58-a8bf-f6f0fba78f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### make document enrichment work without having to pass the prior embedding layer! maybe through decision or concatenation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9612c963-ee3f-4c12-88fa-0358602ff637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\lib\\site-packages\\pygraphviz\\agraph.py:1405: RuntimeWarning: Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n",
      "C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\lib\\site-packages\\pygraphviz\\agraph.py:1405: RuntimeWarning: Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "Warning: Could not load \"C:\\Users\\Joel\\anaconda3\\envs\\MasterNLP\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "p_embeddings.draw()\n",
    "# display the saved image in the notebook\n",
    "Image(filename=\"pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ef295-92d9-4406-afd7-1aad904ff438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c549b6-074f-4c21-892f-6c7c2f8240ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59cc36-28c8-40bd-ad8b-04c1a933a039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda3cfd-7545-42d1-a62f-9f2430c5aa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f51367-8b46-4166-bdb6-d602f1dbc263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312d03d-428c-4497-a80d-9ed41c8d0cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f8d9570-c60c-4b84-bf96-337a97420314",
   "metadata": {},
   "source": [
    "# Testings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5697a014-485d-4e23-aaee-bf652f1eb92a",
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineConfigError",
     "evalue": "The first node of a pipeline must have one single root node as input (Query or File).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPipelineConfigError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#pipeline.add_node(component, name=\"custom_component\", requires=[\"text\"], produces=[\"custom_features\"])\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom_component\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# You can then execute the pipeline on input text as follows:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m result \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mrun(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy input text to process with custom component\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\haystack\\pipelines\\base.py:406\u001b[0m, in \u001b[0;36mPipeline.add_node\u001b[1;34m(self, component, name, inputs)\u001b[0m\n\u001b[0;32m    404\u001b[0m     candidate_roots \u001b[38;5;241m=\u001b[39m [input_node \u001b[38;5;28;01mfor\u001b[39;00m input_node \u001b[38;5;129;01min\u001b[39;00m inputs \u001b[38;5;28;01mif\u001b[39;00m input_node \u001b[38;5;129;01min\u001b[39;00m VALID_ROOT_NODES]\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(candidate_roots) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 406\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PipelineConfigError(\n\u001b[0;32m    407\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first node of a pipeline must have one single root node \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas input (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(VALID_ROOT_NODES)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         )\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m _init_pipeline_graph(root_node_name\u001b[38;5;241m=\u001b[39mcandidate_roots[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    412\u001b[0m \u001b[38;5;66;03m# Check for duplicate names before adding the component\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;66;03m# Note that the very same component must be addable multiple times:\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# E.g. for indexing pipelines it's common to add a retriever first and a document store afterwards.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# The document store is already being used by the retriever however.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# Thus the very same document store will be added twice, first as a subcomponent of the retriever and second as a first level node.\u001b[39;00m\n",
      "\u001b[1;31mPipelineConfigError\u001b[0m: The first node of a pipeline must have one single root node as input (Query or File)."
     ]
    }
   ],
   "source": [
    "from haystack import BaseComponent\n",
    "from typing import Optional, List\n",
    "\n",
    "class CustomComponent(BaseComponent):\n",
    "    # Number of outputs. >1 only required, if we have a decision node\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def run(self, query: str, my_optional_param: Optional[int]):\n",
    "        # process the inputs\n",
    "        output = {\"my_output\": \"out1\"}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(self, queries: List[str], my_optional_param: Optional[int]):\n",
    "        # process the inputs\n",
    "        output = {\"my_output\": \"out1\"}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "component = CustomComponent()\n",
    "\n",
    "pipeline = Pipeline()\n",
    "#pipeline.add_node(component, name=\"custom_component\", requires=[\"text\"], produces=[\"custom_features\"])\n",
    "pipeline.add_node(component, name=\"custom_component\", inputs=[\"text\"])\n",
    "\n",
    "# You can then execute the pipeline on input text as follows:\n",
    "result = pipeline.run(query=\"My input text to process with custom component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fbb628-9b3b-410c-9b65-6aeaffce247a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
